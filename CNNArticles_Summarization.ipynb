{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generative AI- Worked Example**\n",
        "\n",
        "1. **Understand the Concepts**\n",
        "\n",
        "\n",
        "The model_name='google/flan-t5-base' refers to a specific pre-trained model available from Google Research, which is part of the family of models built on the T5 (Text-to-Text Transfer Transformer) architecture. This particular version is integrated with an approach known as Flan-T5. Let’s break down what this means and why it's significant.\n",
        "\n",
        "T5 (Text-to-Text Transfer Transformer)\n",
        "T5 was developed by Google Research and published in a paper titled \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" in 2019. The core idea behind T5 is to treat every text-based language task as a \"text-to-text\" problem. This means that whether the task is translation, summarization, question answering, or even classification, the input and output are always treated as text strings.\n",
        "\n",
        "Input and Output: Everything is treated uniformly as text. For example, for a classification task, the output would be the class name in text form.\n",
        "Architecture: T5 is built on the transformer model, which relies on self-attention mechanisms and has been highly effective for a range of natural language processing tasks.\n",
        "Flan-T5 (Fine-tuned Language Net)\n",
        "Flan-T5, introduced by Google AI in a paper titled \"Fine-tuned Language Models Are Zero-Shot Learners,\" builds on the foundation of T5. This model variation focuses on enhancing the zero-shot and few-shot learning capabilities of T5 through a process called \"instruction tuning.\"\n",
        "\n",
        "Instruction Tuning: Instead of training on task-specific data, Flan-T5 is fine-tuned using a mix of datasets framed with instructions. This means that during training, the model is exposed to various tasks presented with explicit instructions, improving its ability to understand and execute language tasks based directly on prompts, even without prior task-specific fine-tuning.\n",
        "Flexibility and Generality: The instruction tuning makes Flan-T5 particularly good at handling tasks described in natural language, enabling it to adapt to a variety of tasks with minimal task-specific data. This versatility makes it a strong candidate for applications requiring multi-task capabilities and robustness across diverse language tasks.\n",
        "Usage in Practice\n",
        "When you use model_name='google/flan-t5-base' with libraries like transformers from Hugging Face, you are accessing a pre-trained version of this model. It's designed to be flexible and effective across a range of tasks without the need for extensive additional training:\n",
        "\n",
        "Plug-and-Play: Due to its training regime, Flan-T5 can be employed in applications with limited examples (few-shot learning) or even in scenarios where no task-specific examples are provided (zero-shot learning).\n",
        "Performance: It generally offers strong performance across diverse natural language understanding and generation tasks.\n",
        "In summary, by specifying google/flan-t5-base, you're choosing a powerful and flexible model pre-trained to handle tasks based on text instructions, making it ideal for many NLP applications right out of the box.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y5rhAp-gqoXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Modifications:** When considering modifications or improvements to an NLP model's performance, switching or combining datasets is a common strategy. It can significantly affect how well the model generalizes across different tasks. Here, you're looking at two specific datasets, ccdv/pubmed-summarization and ccdv/cnn_dailymail. Let's explore what each dataset offers and how using them could impact your model's training and performance, particularly in tasks like summarization.\n",
        "\n",
        "1. ccdv/pubmed-summarization\n",
        "This dataset is geared towards scientific literature, particularly articles from PubMed, a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The key characteristics of this dataset include:\n",
        "\n",
        "Content Type: It consists of complex, technical language and extensive use of domain-specific terminology.\n",
        "Use Cases: It is ideal for training models that are expected to perform tasks involving scientific texts, like extracting findings from studies or summarizing research articles.\n",
        "2. ccdv/cnn_dailymail\n",
        "This dataset is based on news articles from CNN and the Daily Mail. It is one of the most popular datasets for training and benchmarking models on the summarization task. The dataset has the following features:\n",
        "\n",
        "Content Type: The language is less technical than the PubMed dataset and covers a broad range of topics typically found in news articles.\n",
        "Use Cases: Perfect for models intended to summarize news articles, which require an understanding of general-world knowledge and the ability to condense information into a few sentences.\n",
        "Modifications or Improvements Using These Datasets\n",
        "Enhancing Domain Adaptability: If your model initially trained on general text, using the ccdv/pubmed-summarization dataset can enhance its ability to handle scientific literature. This is particularly useful if your application involves working with academic or clinical documentation.\n",
        "\n",
        "Improving Summarization Skills: Training on ccdv/cnn_dailymail can refine the model's ability to generate concise summaries from longer texts, a valuable skill in many applications beyond just news summarization, such as executive summaries for business documents or reducing lengthy emails to essential content.\n",
        "\n",
        "Combined Training: Depending on your final application, you might consider combining both datasets in training. This approach can help develop a model that is robust across different domains, capable of handling both highly technical texts and more general news articles.\n",
        "\n",
        "Cross-Domain Validation: Training on one dataset and validating on another can help you assess how well your model generalizes across different types of text. For instance, a model trained on ccdv/cnn_dailymail might be tested on ccdv/pubmed-summarization to see how well it adapts to technical summarization tasks without further training.\n",
        "\n",
        "Implementing the Dataset Change\n",
        "If you decide to switch or combine these datasets for training, consider how to integrate them smoothly:\n",
        "\n",
        "Preprocessing: Adapt the preprocessing steps to handle differences in text structure and content between the datasets.\n",
        "Balancing: When combining datasets, ensure the model doesn't become biased towards the style or content of one dataset over the other.\n",
        "Evaluation Metrics: Use appropriate metrics that can accurately reflect the performance in summarization tasks for both scientific and general texts.\n",
        "Switching to these datasets or combining them can significantly enhance the model's versatility and effectiveness in summarization tasks, tailored to the specific demands of your application.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MhwR79alr3sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Documentation"
      ],
      "metadata": {
        "id": "f3tNoFl-rILZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNvKjUGtMCdO",
        "outputId": "4ae23bff-b38c-4d20-ff92-2f0abd17e111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.27.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.43.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0  --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDmIKgNdW034",
        "outputId": "1c897db0-c187-4a49-fc1e-3245fd04e92e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "L-q9HwjUX00Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "huggingface_dataset_name = \"ccdv/cnn_dailymail\"\n",
        "config_name = \"1.0.0\"  # You can choose '1.0.0' or '2.0.0' as well\n",
        "dataset = load_dataset(huggingface_dataset_name, config_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCwuWumYW1Dx",
        "outputId": "20659de7-087c-483f-9259-47a55c0b9817"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for ccdv/cnn_dailymail contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ccdv/cnn_dailymail\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  Set the features type to use for this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcUxvIhdYF1p",
        "outputId": "98fb7d5d-5d1d-46d2-9080-ac65873db8ce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 287113\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 13368\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 11490\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Assuming 'dataset' is your existing DatasetDict with the 'train', 'validation', and 'test' splits\n",
        "\n",
        "# Code to reduce the number of rows to 100 for each split\n",
        "for split in dataset.keys():\n",
        "    # Select first 100 indices for the split\n",
        "    dataset[split] = dataset[split].select(range(100))\n",
        "\n",
        "# The 'dataset' variable now contains only the first 100 examples of each split"
      ],
      "metadata": {
        "id": "u3xfHgTEW1N5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N7oK0g3W1Ym",
        "outputId": "6565e2b2-25e8-48c1-ef42-23811c7fd333"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'dataset' is your DatasetDict\n",
        "new_dataset = {}\n",
        "\n",
        "for split in dataset.keys():\n",
        "    # Select first 100 indices for the split\n",
        "    new_dataset[split] = dataset[split].select(range(100))\n",
        "\n",
        "# Now new_dataset contains only the first 100 examples of each split\n"
      ],
      "metadata": {
        "id": "VwQE7p1eXOKw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [40, 89]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print('INPUT ARTICLE:')\n",
        "    print(dataset['test'][index]['article'])\n",
        "    print(dash_line)\n",
        "    print('BASELINE ABSTRACT:')\n",
        "    print(dataset['test'][0].keys())\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkbZ14oDZeO5",
        "outputId": "18c85d58-4ca6-4347-9ae6-427af166522e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT ARTICLE:\n",
            "(CNN)Editor's Note: Ines Dumig was recently announced as a CENTER Grant Recipient. Sahra, a Somali refugee, left her home at 14 years old. Throughout her journey in search of asylum, she managed to overcome dangers and discomforts. But she never gave up, and she continuously reminded herself to keep going. She's the focus of Ines Dumig's photo series \"Apart Together.\" Dumig met Sahra through a photo workshop at Refugio, a shelter in Munich, Germany, for refugees and torture victims. What drew Dumig to Sahra specifically was her strength and her ability to effectively reflect on all of her experiences. \"It really impressed me how she deals with everything,\" Dumig said. \"She's strong in her way of connecting with the culture here and also reflecting on what happened, the culture where she comes from.\" The number of refugees seeking asylum in the European Union increased by 25% last year, with Germany receiving the most applications. One of the reasons Dumig decided to photograph Sahra is because growing up in Germany made Dumig realize that she lived a fortunate lifestyle. Another reason has to do with Dumig's interest in people's emotions and finding one's identity. \"I realized so many people want to come to Europe, and I always had the feeling to disappear or to go away,\" Dumig said. \"Seeing how people live in other parts of the world made me realize how privileged I am.\" \"Apart Together\" serves not only as a documentation of Sahra, but as a far-reaching story about people from all backgrounds. The title of Dumig's work refers to the fact that although people may be physically apart from one another, the comparable feelings they experience are what link all people together. \"Sometimes we feel strong, sometimes we feel lost -- that's kind of universal, I think,\" Dumig said. \"That's why I want to universalize (Sahra's) story as well, not only make it about her.\" The underlying themes of \"Apart Together\" include the feelings of isolation and \"otherness\" and the search for a valuable human dignity. Social media . Follow @CNNPhotos on Twitter to join the conversation about photography. \"Every one of these (refugees) have strong stories, and in the bureaucratic system, they are just a number or a document,\" Dumig said. \"But they are a person, they are people with emotions and lives.\" Sahra is currently under the status of \"suspension of deportation,\" meaning German immigration officials may grant her discretionary relief from deportation. Dumig describes Sahra as someone living through an unresolved situation. Regardless of the challenges Sahra faces as a refugee in Germany, she is a survivor and the embodiment of resilience, determined to establish a new life for herself. She has learned to speak German fluently, and she has started working in the nation as well. Like an unsolved photographic puzzle, each photo within \"Apart Together\" provides a piece of insight into Sahra's experiences. There is no certain and clear way in which to arrange the pieces, because they are a representation of the fragmented nature of Sahra's life. Many of Dumig's photos are not of Sahra herself, but instead show her surroundings. This makes \"Apart Together\" rich in symbolism and challenges viewers to develop their own perceptions. The photos are powerful because of this symbolic nature, as there are infinite interpretations attached to each one. \"I think everyone interprets by themselves, by however way they perceive it through their own experience. That's up to the viewer,\" Dumig said. \"It depends on who looks at the pictures. ... Everyone will see something different.\" \"Apart Together\" allowed Dumig to share various special moments with Sahra, and they were both able to learn from each other. \"It was just something we both got something out of,\" Dumig said. Ines Dumig is a photographer based in Germany.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE ABSTRACT:\n",
            "dict_keys(['article', 'highlights', 'id'])\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT ARTICLE:\n",
            "(CNN)Gastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday. The Infinity left San Diego on March 29. It made its last stop in Puerto Vallarta, Mexico, on April 10, according to MarineTraffic.com. Celebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says. According to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013. The ship was built in 2001 and refurbished in 2011.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE ABSTRACT:\n",
            "dict_keys(['article', 'highlights', 'id'])\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDq1FFeMav6k",
        "outputId": "4b4a3380-a1fc-406e-e4a1-0e2d77428e47"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "NJUq0P8WazuL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "vR7p3jhOa4sV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "arFkP_oQbJXI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What time is it?\"\n",
        "\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('ENCODED SENTENCE:')\n",
        "print(sentence_encoded[\"input_ids\"][0])\n",
        "print('\\nDECODED SENTENCE:')\n",
        "print(sentence_decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhLu6KG8bXLb",
        "outputId": "cab3143f-d053-47d9-a76a-e7cf884bda1c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCODED SENTENCE:\n",
            "tensor([363,  97,  19,  34,  58,   1])\n",
            "\n",
            "DECODED SENTENCE:\n",
            "What time is it?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the keys of the first entry in the test dataset to confirm structure\n",
        "print(dataset['test'][0].keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8GV-cB9he95",
        "outputId": "2298aa6c-5a16-49ba-e8a2-8c7683a28259"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['article', 'highlights', 'id'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    article = dataset['test'][index]['article']\n",
        "    highlights = dataset['test'][index]['highlights']  # Use 'highlights' instead of 'abstract'\n",
        "\n",
        "    inputs = tokenizer(article, return_tensors='pt')\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print('-' * 80)\n",
        "    print(f'Example {i + 1}')\n",
        "    print('-' * 80)\n",
        "    print(f'INPUT ARTICLE:\\n{article}')\n",
        "    print('-' * 80)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{highlights}')\n",
        "    print('-' * 80)\n",
        "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{summary}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epsksJ70baS5",
        "outputId": "5098d102-dc23-47f6-b630-675c389fd549"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (922 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Example 1\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT ARTICLE:\n",
            "(CNN)Editor's Note: Ines Dumig was recently announced as a CENTER Grant Recipient. Sahra, a Somali refugee, left her home at 14 years old. Throughout her journey in search of asylum, she managed to overcome dangers and discomforts. But she never gave up, and she continuously reminded herself to keep going. She's the focus of Ines Dumig's photo series \"Apart Together.\" Dumig met Sahra through a photo workshop at Refugio, a shelter in Munich, Germany, for refugees and torture victims. What drew Dumig to Sahra specifically was her strength and her ability to effectively reflect on all of her experiences. \"It really impressed me how she deals with everything,\" Dumig said. \"She's strong in her way of connecting with the culture here and also reflecting on what happened, the culture where she comes from.\" The number of refugees seeking asylum in the European Union increased by 25% last year, with Germany receiving the most applications. One of the reasons Dumig decided to photograph Sahra is because growing up in Germany made Dumig realize that she lived a fortunate lifestyle. Another reason has to do with Dumig's interest in people's emotions and finding one's identity. \"I realized so many people want to come to Europe, and I always had the feeling to disappear or to go away,\" Dumig said. \"Seeing how people live in other parts of the world made me realize how privileged I am.\" \"Apart Together\" serves not only as a documentation of Sahra, but as a far-reaching story about people from all backgrounds. The title of Dumig's work refers to the fact that although people may be physically apart from one another, the comparable feelings they experience are what link all people together. \"Sometimes we feel strong, sometimes we feel lost -- that's kind of universal, I think,\" Dumig said. \"That's why I want to universalize (Sahra's) story as well, not only make it about her.\" The underlying themes of \"Apart Together\" include the feelings of isolation and \"otherness\" and the search for a valuable human dignity. Social media . Follow @CNNPhotos on Twitter to join the conversation about photography. \"Every one of these (refugees) have strong stories, and in the bureaucratic system, they are just a number or a document,\" Dumig said. \"But they are a person, they are people with emotions and lives.\" Sahra is currently under the status of \"suspension of deportation,\" meaning German immigration officials may grant her discretionary relief from deportation. Dumig describes Sahra as someone living through an unresolved situation. Regardless of the challenges Sahra faces as a refugee in Germany, she is a survivor and the embodiment of resilience, determined to establish a new life for herself. She has learned to speak German fluently, and she has started working in the nation as well. Like an unsolved photographic puzzle, each photo within \"Apart Together\" provides a piece of insight into Sahra's experiences. There is no certain and clear way in which to arrange the pieces, because they are a representation of the fragmented nature of Sahra's life. Many of Dumig's photos are not of Sahra herself, but instead show her surroundings. This makes \"Apart Together\" rich in symbolism and challenges viewers to develop their own perceptions. The photos are powerful because of this symbolic nature, as there are infinite interpretations attached to each one. \"I think everyone interprets by themselves, by however way they perceive it through their own experience. That's up to the viewer,\" Dumig said. \"It depends on who looks at the pictures. ... Everyone will see something different.\" \"Apart Together\" allowed Dumig to share various special moments with Sahra, and they were both able to learn from each other. \"It was just something we both got something out of,\" Dumig said. Ines Dumig is a photographer based in Germany.\n",
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ines Dumig's photo series \"Apart Together\" follows a Somali refugee living in Germany . The underlying themes include isolation and \"otherness\" and the search for human dignity .\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Ines Dumig's \"Apart Together\" series is a portrait of a refugee in Germany. Sahra, a Somali refugee, left her home at 14 years old. Dumig met Sahra\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Example 2\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT ARTICLE:\n",
            "(CNN)Gastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday. The Infinity left San Diego on March 29. It made its last stop in Puerto Vallarta, Mexico, on April 10, according to MarineTraffic.com. Celebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says. According to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013. The ship was built in 2001 and refurbished in 2011.\n",
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "100 passengers and crew members have been sickened on Celebrity Infinity . The ship, which is based on the West Coast, left San Diego in late March . The CDC is scheduled to board the ship Monday .\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Celebrity Infinity is the third ship to suffer an outbreak of gastrointestinal illness, with another three in 2006.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Output**\n",
        "\n",
        "\n",
        "Analyzing the output from your code snippet, which involves generating summaries using a model, we can discuss the performance of the model in relation to the baseline human summaries provided in the dataset. This can help identify if there are improvements needed in model training, prompt engineering, or other aspects.\n",
        "\n",
        "Analysis of Example 1\n",
        "Input Article: A detailed description of a photo series by Ines Dumig focusing on Sahra, a Somali refugee in Germany, showcasing her challenges and the broader themes of isolation and otherness.\n",
        "\n",
        "Baseline Human Summary: Condenses the story to focus on the thematic elements of the photo series, highlighting the main subject (Sahra) and the overarching themes (isolation, otherness, human dignity).\n",
        "\n",
        "Model-Generated Summary: Begins to describe the same photo series but seems to cut off mid-sentence. This indicates that the model might be either generating too verbose a beginning, causing it to hit a token limit before completing the summary, or the max_new_tokens parameter may need adjusting to allow more complete thoughts.\n",
        "\n",
        "Analysis of Example 2\n",
        "Input Article: Discusses a gastrointestinal illness outbreak on the cruise ship Celebrity Infinity, detailing the number of affected individuals and the response measures.\n",
        "\n",
        "Baseline Human Summary: Provides a succinct summary focusing on the number of affected individuals, the ship's itinerary, and the CDC’s involvement.\n",
        "\n",
        "Model-Generated Summary: The model captures the recurrent theme of gastrointestinal outbreaks on the ship but does not accurately reflect the details of the current incident. It mentions the previous occurrences (2006, 2013) but truncates the specific details about the current outbreak and does not mention the CDC's planned actions.\n",
        "\n",
        "Observations and Recommendations:\n",
        "Summary Completeness: The model-generated summaries should ideally capture the most relevant and current details of the articles. The summaries should be complete sentences or thoughts, especially when summarizing complex content like in Example 1. Consider adjusting the max_new_tokens or reviewing if the model is truncating outputs prematurely.\n",
        "\n",
        "Relevance and Accuracy: Ensure that the summaries are not only concise but also accurate reflections of the primary content. In Example 2, the model touches on historical context but fails to provide actionable or current information, which is more relevant for a summary of a news article.\n",
        "\n",
        "Prompt Engineering: Consider refining the prompt structure or the instructions given to the model to focus more on the critical elements of the articles. This might involve more explicitly directing the model on what to focus on in the summary.\n",
        "\n",
        "Model Configuration: Depending on the observed issues, consider experimenting with different configurations of the model's parameters, such as increasing num_beams for better quality outputs or adjusting the no_repeat_ngram_size to avoid unnecessary repetition and promote more diverse language usage.\n",
        "\n",
        "Further Training: If the model consistently underperforms on similar types of content, additional fine-tuning with more representative data of the target summarization task might be required.\n",
        "\n",
        "By addressing these areas, you can enhance the model's performance to ensure that the summaries it generates are both informative and reflective of the critical contents of the articles.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LAImf6AtHF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    article = dataset['test'][index]['article']\n",
        "    highlights = dataset['test'][index]['highlights']  # Replace 'abstract' with 'highlights'\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Summarize what they are talking about.\n",
        "\n",
        "{article}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the article.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print('-' * 80)\n",
        "    print('Example ', i + 1)\n",
        "    print('-' * 80)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print('-' * 80)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{highlights}')  # Change 'abstract' to 'highlights'\n",
        "    print('-' * 80)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qbI8h4PbnwC",
        "outputId": "1c166652-6d94-4767-b760-e9cb7b335543"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Example  1\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Summarize what they are talking about.\n",
            "\n",
            "(CNN)Editor's Note: Ines Dumig was recently announced as a CENTER Grant Recipient. Sahra, a Somali refugee, left her home at 14 years old. Throughout her journey in search of asylum, she managed to overcome dangers and discomforts. But she never gave up, and she continuously reminded herself to keep going. She's the focus of Ines Dumig's photo series \"Apart Together.\" Dumig met Sahra through a photo workshop at Refugio, a shelter in Munich, Germany, for refugees and torture victims. What drew Dumig to Sahra specifically was her strength and her ability to effectively reflect on all of her experiences. \"It really impressed me how she deals with everything,\" Dumig said. \"She's strong in her way of connecting with the culture here and also reflecting on what happened, the culture where she comes from.\" The number of refugees seeking asylum in the European Union increased by 25% last year, with Germany receiving the most applications. One of the reasons Dumig decided to photograph Sahra is because growing up in Germany made Dumig realize that she lived a fortunate lifestyle. Another reason has to do with Dumig's interest in people's emotions and finding one's identity. \"I realized so many people want to come to Europe, and I always had the feeling to disappear or to go away,\" Dumig said. \"Seeing how people live in other parts of the world made me realize how privileged I am.\" \"Apart Together\" serves not only as a documentation of Sahra, but as a far-reaching story about people from all backgrounds. The title of Dumig's work refers to the fact that although people may be physically apart from one another, the comparable feelings they experience are what link all people together. \"Sometimes we feel strong, sometimes we feel lost -- that's kind of universal, I think,\" Dumig said. \"That's why I want to universalize (Sahra's) story as well, not only make it about her.\" The underlying themes of \"Apart Together\" include the feelings of isolation and \"otherness\" and the search for a valuable human dignity. Social media . Follow @CNNPhotos on Twitter to join the conversation about photography. \"Every one of these (refugees) have strong stories, and in the bureaucratic system, they are just a number or a document,\" Dumig said. \"But they are a person, they are people with emotions and lives.\" Sahra is currently under the status of \"suspension of deportation,\" meaning German immigration officials may grant her discretionary relief from deportation. Dumig describes Sahra as someone living through an unresolved situation. Regardless of the challenges Sahra faces as a refugee in Germany, she is a survivor and the embodiment of resilience, determined to establish a new life for herself. She has learned to speak German fluently, and she has started working in the nation as well. Like an unsolved photographic puzzle, each photo within \"Apart Together\" provides a piece of insight into Sahra's experiences. There is no certain and clear way in which to arrange the pieces, because they are a representation of the fragmented nature of Sahra's life. Many of Dumig's photos are not of Sahra herself, but instead show her surroundings. This makes \"Apart Together\" rich in symbolism and challenges viewers to develop their own perceptions. The photos are powerful because of this symbolic nature, as there are infinite interpretations attached to each one. \"I think everyone interprets by themselves, by however way they perceive it through their own experience. That's up to the viewer,\" Dumig said. \"It depends on who looks at the pictures. ... Everyone will see something different.\" \"Apart Together\" allowed Dumig to share various special moments with Sahra, and they were both able to learn from each other. \"It was just something we both got something out of,\" Dumig said. Ines Dumig is a photographer based in Germany.\n",
            "\n",
            "Summary:\n",
            "    \n",
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ines Dumig's photo series \"Apart Together\" follows a Somali refugee living in Germany . The underlying themes include isolation and \"otherness\" and the search for human dignity .\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Ines Dumig's \"Apart Together\" is a collection of photos of refugees in Germany.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Example  2\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Summarize what they are talking about.\n",
            "\n",
            "(CNN)Gastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday. The Infinity left San Diego on March 29. It made its last stop in Puerto Vallarta, Mexico, on April 10, according to MarineTraffic.com. Celebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says. According to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013. The ship was built in 2001 and refurbished in 2011.\n",
            "\n",
            "Summary:\n",
            "    \n",
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "100 passengers and crew members have been sickened on Celebrity Infinity . The ship, which is based on the West Coast, left San Diego in late March . The CDC is scheduled to board the ship Monday .\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "The CDC says a gastrointestinal illness has gripped the ship Celebrity Infinity.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t2RO21YZtyK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explaination of Code*\n",
        "\n",
        "This code iterates through a set of example indices from a test dataset, preparing and using each corresponding article to generate summaries with a pre-trained language model. It begins by retrieving each article and its associated highlights (serving as the baseline summary). The code constructs a specific input prompt for the model that cues it to summarize the article. This prompt includes an instruction followed by the article text and a placeholder for the summary. Using the Transformers library, the prompt is tokenized and fed into the model, which then generates a summary limited to 50 new tokens. The generated summary and the baseline summary are then printed alongside the input prompt for each article, facilitating a comparison between the model's zero-shot generation capabilities and the human-created summary. This process is repeated for each article in the specified example indices, aiming to evaluate the model's effectiveness in summarizing diverse content without any fine-tuning specifically tailored to the content or task at hand.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6TcaYzhUtb28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaination of the Output:**\n",
        "\n",
        "\n",
        "The effectiveness of the model-generated summaries in the two examples can be evaluated based on how well they capture the essential information of the articles and adhere to the key points outlined in the baseline human summaries. Here’s a detailed analysis of each example:\n",
        "\n",
        "Example 1: Ines Dumig's Photo Series \"Apart Together\"\n",
        "Input Prompt: The article provided a detailed account of Ines Dumig’s photo series focusing on Sahra, a Somali refugee in Germany, emphasizing the themes of isolation, otherness, and the search for human dignity through photographic storytelling.\n",
        "\n",
        "Baseline Human Summary: Succinctly summarizes the photo series by highlighting the subject (a Somali refugee), the location (Germany), and the thematic focus (isolation, otherness, human dignity).\n",
        "\n",
        "Model Generation - Zero Shot: Provides a very brief overview, noting that \"Apart Together\" is a collection of photos of refugees in Germany. While this summary is accurate in terms of subject matter, it is overly simplistic and misses key thematic elements such as isolation, otherness, and the deeper emotional and symbolic content discussed in the article.\n",
        "\n",
        "Effectiveness: The model-generated summary for Example 1 is partially effective. It correctly identifies the subject of the photo series but fails to convey the depth or the thematic richness described in the article and highlighted in the human summary. It lacks detail on the emotional and cultural exploration which is central to understanding the impact of Dumig's work.\n",
        "\n",
        "Example 2: Gastrointestinal Illness on Celebrity Infinity\n",
        "Input Prompt: This article discusses an outbreak of gastrointestinal illness affecting passengers and crew on the cruise ship Celebrity Infinity, detailing the response and historical context of similar past outbreaks.\n",
        "\n",
        "Baseline Human Summary: Efficiently summarizes the key facts: the number affected, the ship’s location, and the impending CDC involvement.\n",
        "\n",
        "Model Generation - Zero Shot: Simplifies the event to a single sentence stating that the CDC reported a gastrointestinal illness on the ship. This summary correctly captures the main event (illness on the ship) but omits significant details such as the number of people affected, the response measures taken, and the CDC's scheduled visit.\n",
        "\n",
        "Effectiveness: The model-generated summary for Example 2 is accurate but lacks depth. It mentions the main issue but leaves out critical details that are valuable for a full understanding of the situation, such as the scale of the outbreak and the actions taken by the authorities and the cruise line.\n",
        "\n",
        "Overall Assessment\n",
        "The model seems to have a tendency to oversimplify, which can be attributed to the zero-shot generation setup where the model has not been fine-tuned on specific summarization tasks or perhaps the brief limit of 50 new tokens might be constraining more comprehensive summaries. To improve effectiveness, the model could benefit from prompt engineering that more explicitly asks for thematic and detailed summaries or adjusting the token limit if possible. Additionally, training or fine-tuning on specific summarization tasks or datasets involving complex thematic content could help enhance the model's ability to generate more nuanced and informative summaries.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XwHVBx3etjfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(dataset, example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        article = dataset['test'][index]['article']\n",
        "        abstract = dataset['test'][index]['abstract']\n",
        "\n",
        "        # The stop sequence '{abstract}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
        "        prompt += f\"\"\"\n",
        "Article:\n",
        "\n",
        "{article}\n",
        "\n",
        "Abstract of the given article\n",
        "{abstract}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    article = dataset['test'][example_index_to_summarize]['article']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Article\n",
        "\n",
        "{article}\n",
        "\n",
        "Abstract of the given article\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "Wp1fjm7cbv0K"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(dataset, example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        article = dataset['test'][index]['article']\n",
        "        highlights = dataset['test'][index]['highlights']\n",
        "\n",
        "        # Append article and highlights to the prompt\n",
        "        prompt += f\"\"\"\n",
        "Article:\n",
        "\n",
        "{article}\n",
        "\n",
        "Highlights:\n",
        "{highlights}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    article_to_summarize = dataset['test'][example_index_to_summarize]['article']\n",
        "    highlights_to_summarize = dataset['test'][example_index_to_summarize]['highlights']\n",
        "\n",
        "    # Append the article and highlights for the example to be summarized\n",
        "    prompt += f\"\"\"\n",
        "Article:\n",
        "\n",
        "{article_to_summarize}\n",
        "\n",
        "Highlights:\n",
        "{highlights_to_summarize}\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "v1l2WF1ZbzWx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Assuming 'dataset' is your existing DatasetDict with the 'train', 'validation', and 'test' splits\n",
        "\n",
        "# Code to reduce the number of rows to 100 for each split\n",
        "for split in dataset.keys():\n",
        "    # Select first 100 indices for the split\n",
        "    dataset[split] = dataset[split].select(range(100))\n",
        "\n",
        "# Reinitialize or reload the dataset to update the metadata\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset['train'],\n",
        "    'validation': dataset['validation'],\n",
        "    'test': dataset['test']\n",
        "})\n",
        "\n",
        "# The 'dataset' variable now contains only the first 100 examples of each split with updated metadata\n",
        "print(dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eHq2nvWb4wC",
        "outputId": "cf60a8a0-b8eb-42aa-f997-9cc9c09f82ee"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [20, 40, 80]\n",
        "example_index_to_summarize = 99\n",
        "\n",
        "few_shot_prompt = make_prompt(dataset, example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtIN_jihb7qh",
        "outputId": "7f98704e-a4a0-4e2d-cd7f-09fc49560206"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarize: (CNN)James Best, best known for his portrayal of bumbling sheriff Rosco P. Coltrane on TV's \"The Dukes of Hazzard,\" died Monday after a brief illness. He was 88. Best died in hospice in Hickory, North Carolina, of complications from pneumonia, said Steve Latshaw, a longtime friend and Hollywood colleague. Although he'd been a busy actor for decades in theater and in Hollywood, Best didn't become famous until 1979, when \"The Dukes of Hazzard's\" cornpone charms began beaming into millions of American homes almost every Friday night. For seven seasons, Best's Rosco P. Coltrane chased the moonshine-running Duke boys back and forth across the back roads of fictitious Hazzard County, Georgia, although his \"hot pursuit\" usually ended with him crashing his patrol car. Although Rosco was slow-witted and corrupt, Best gave him a childlike enthusiasm that got laughs and made him endearing. His character became known for his distinctive \"kew-kew-kew\" chuckle and for goofy catchphrases such as \"cuff 'em and stuff 'em!\" upon making an arrest. Among the most popular shows on TV in the early '80s, \"The Dukes of Hazzard\" ran until 1985 and spawned TV movies, an animated series and video games. Several of Best's \"Hazzard\" co-stars paid tribute to the late actor on social media. \"I laughed and learned more from Jimmie in one hour than from anyone else in a whole year,\" co-star John Schneider, who played Bo Duke, said on Twitter. \"Give Uncle Jesse my love when you see him dear friend.\" \"Jimmy Best was the most constantly creative person I have ever known,\" said Ben Jones, who played mechanic Cooter on the show, in a Facebook post. \"Every minute of his long life was spent acting, writing, producing, painting, teaching, fishing, or involved in another of his life's many passions.\" Born Jewel Guy on July 26, 1926, in Powderly, Kentucky, Best was orphaned at 3 and adopted by Armen and Essa Best, who renamed him James and raised him in rural Indiana. Best served in the Army during World War II before launching his acting career. In the 1950s and 1960s, he accumulated scores of credits, playing a range of colorful supporting characters in such TV shows as \"The Twilight Zone,\" \"Bonanza,\" \"The Andy Griffith Show\" and \"Gunsmoke.\" He later appeared in a handful of Burt Reynolds' movies, including \"Hooper\" and \"The End.\" But Best will always be best known for his \"Hazzard\" role, which lives on in reruns. \"Jimmie was my teacher, mentor, close friend and collaborator for 26 years,\" Latshaw said. \"I directed two of his feature films, including the recent 'Return of the Killer Shrews,' a sequel he co-wrote and was quite proud of as he had made the first one more than 50 years earlier.\" People we've lost in 2015 . CNN's Stella Chan contributed to this story.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explaination of the Code***\n",
        "\n",
        "The code you've provided accomplishes several tasks related to data manipulation and preparation for a few-shot learning scenario using a language model. Here’s a breakdown of its main components and functionalities:\n",
        "\n",
        "Dataset Reduction: The script starts by reducing each split (train, validation, test) of a DatasetDict to the first 100 entries. This is useful for testing or demonstration purposes where operating on a smaller dataset might be necessary due to resource constraints or for quicker iterations during development.\n",
        "\n",
        "Reinitialization of Dataset: After reducing the size of each split, the dataset is reinitialized. This step is crucial because it updates the internal state of the dataset to reflect the changes, such as the reduced number of entries. This ensures that any operations on the dataset, such as indexing, are performed on the updated data structure.\n",
        "\n",
        "Prompt Construction for Few-Shot Learning: The script defines a function make_prompt which is used to construct a text prompt for a few-shot learning scenario. This function takes a dataset and a list of indices (examples for few-shot context) and the index of the article to summarize. The function constructs a prompt that includes several examples followed by the article to be summarized without its abstract. This structured prompt is designed to guide the model by showing it examples of the input (articles) followed by the desired output (summaries), before giving it a new article to generate a summary for, without providing the summary.\n",
        "\n",
        "Application of make_prompt Function: The function is then called with specified indices to generate a prompt for few-shot learning. This prompt is intended to be used with a model like FLAN-T5, which is designed to perform well in few-shot scenarios by leveraging the examples provided in the prompt.\n",
        "\n",
        "Print Statements: The script includes print statements to display the constructed prompt and verify that the reduction and reinitialization of the dataset were successful.\n",
        "\n",
        "This setup is typically used in scenarios where you need to leverage the capabilities of advanced NLP models to generate text based on a given context, in this case, summaries based on articles. The specific structure of the prompt, including the use of \"Abstract of the given article\" as a cue, is tailored to instruct the model on what task it needs to perform, optimizing its output for summarization tasks in a few-shot learning framework.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "krGbYMZLuGP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Assuming 'dataset' is your original DatasetDict object\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    # Rename 'highlights' to 'abstract'\n",
        "    dataset[split] = dataset[split].rename_column('highlights', 'abstract')\n",
        "\n",
        "    # Remove 'id' column\n",
        "    dataset[split] = dataset[split].remove_columns(['id'])\n",
        "\n",
        "# The original 'dataset' variable is now updated to the new format\n",
        "print(dataset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IqpCBfVo5Bc",
        "outputId": "ba0e3853-dd0b-40ee-f0bb-dbde647487df"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'abstract'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'abstract'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'abstract'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example index from the dataset\n",
        "example_index_to_summarize = 0\n",
        "\n",
        "# Assuming 'dataset' is defined and contains a 'test' subset\n",
        "if 'abstract' in dataset['test'][example_index_to_summarize]:\n",
        "    summary = dataset['test'][example_index_to_summarize]['abstract']\n",
        "    one_shot_prompt = \"Summarize: \" + summary\n",
        "\n",
        "    # Tokenization and Model Inference\n",
        "    inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50  # Limit the generation to 50 tokens\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Output the summaries\n",
        "    print(\"-\" * 50)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "    print(\"-\" * 50)\n",
        "    print(f'MODEL GENERATION - ONE SHOT:\\n{output}')\n",
        "else:\n",
        "    # Handling cases where 'abstract' is not available\n",
        "    print(\"Abstract not available, unable to generate a summary.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmo8gqvdnSye",
        "outputId": "4fd68e91-b84d-4575-ca09-277e3f4c4c50"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88 . \"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV .\n",
            "\n",
            "--------------------------------------------------\n",
            "MODEL GENERATION - ONE SHOT:\n",
            "James Best, who played the sheriff on \"The Dukes of Hazzard,\" died at 88. Best played the sheriff on \"The Dukes of Hazzard\" from 1979 to 1985.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explaination of Code and Output***\n",
        "\n",
        "The provided code checks if an abstract is available in the test subset of a dataset and uses it as the input for a one-shot text generation task using a pre-trained model. The code is structured to perform the following steps:\n",
        "\n",
        "Check for Abstract Availability: The script first checks if the 'abstract' field exists for a given entry in the dataset. If the abstract is present, it proceeds with generating a summary; if not, it prints a message indicating that the abstract is not available.\n",
        "\n",
        "Text Generation: If an abstract is available, the code constructs a prompt by appending the abstract to the phrase \"Summarize:\". This prompt is then tokenized and fed into a pre-trained model to generate a summary. The model's output is restricted to generating a maximum of 50 new tokens to keep the summary concise.\n",
        "\n",
        "Output: The code prints the original human-written summary (baseline) and the machine-generated summary for comparison.\n",
        "\n",
        "Analysis of the Model's Output Compared to the Baseline Human Summary:\n",
        "Baseline Human Summary:\n",
        "\n",
        "Content: The summary mentions James Best's role in \"The Dukes of Hazzard,\" his death at age 88, and notes the show's run and popularity.\n",
        "Style: The summary is concise, informative, and captures key factual elements about James Best and his connection to the show.\n",
        "Model-Generated Summary:\n",
        "\n",
        "Content: The generated summary reiterates James Best's role and his age at death, and repeats the information about his tenure on \"The Dukes of Hazzard.\"\n",
        "Style: The model's output is also concise but introduces a slight redundancy by mentioning twice that Best played the sheriff, which could be seen as inefficient use of space given the character limit.\n",
        "Comparison and Effectiveness:\n",
        "Relevance: Both summaries are relevant and factually accurate, focusing on the most noteworthy aspects of James Best's career.\n",
        "Conciseness: Both summaries are concise, though the model-generated summary could improve by eliminating redundant information.\n",
        "Completeness: The human summary includes the additional context of the show's popularity, which the model-generated summary lacks. This extra detail contributes to a fuller understanding of why James Best might be a significant figure.\n",
        "Conclusion:\n",
        "The model-generated summary is effective in capturing the main factual elements of the human summary but falls short in richness of detail (e.g., omitting the show's popularity). Additionally, the redundancy in the model's output suggests that there might be room for improvement in how the model manages content within the token constraints. Optimizing the model's summarization approach to avoid redundancy and include a broader range of details could enhance its utility for practical applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a9TINCbGudBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load tokenizer and model, make sure they are appropriate for the task\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare a more appropriate prompt with relevant examples\n",
        "few_shot_examples = [\n",
        "    {\"article\": \"Albert Einstein was a theoretical physicist known for the theory of relativity.\", \"summary\": \"Albert Einstein developed the theory of relativity.\"},\n",
        "    {\"article\": \"The Titanic sank after hitting an iceberg in 1912.\", \"summary\": \"The Titanic sank in 1912 due to an iceberg collision.\"}\n",
        "]\n",
        "current_article = {\n",
        "    \"article\": \"James Best, who played the sheriff on 'The Dukes of Hazzard,' died Monday at 88. 'Hazzard' ran from 1979 to 1985 and was among the most popular shows on TV.\",\n",
        "    \"summary\": \"James Best, the sheriff on 'The Dukes of Hazzard,' died at 88.\"\n",
        "}\n",
        "few_shot_prompt = \"\\n\\n\".join([f\"Article: {ex['article']}\\nSummary: {ex['summary']}\" for ex in few_shot_examples + [current_article]])\n",
        "\n",
        "# Prepare inputs\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt', truncation=True, max_length=512)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=512 + 50,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "# Decode the generated ids to text\n",
        "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Clean-up\n",
        "del inputs, generated_ids\n",
        "torch.cuda.empty_cache() if device == 'cuda' else None\n",
        "\n",
        "# Output for evaluation\n",
        "print('-' * 50)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{current_article[\"summary\"]}\\n')\n",
        "print('-' * 50)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-9ozdr6b9yO",
        "outputId": "3aee755f-166a-4fd6-f7e3-d16b9d927fd7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "James Best, the sheriff on 'The Dukes of Hazzard,' died at 88.\n",
            "\n",
            "--------------------------------------------------\n",
            "MODEL GENERATION - FEW SHOT:\n",
            "James Best, the sheriff on 'The Dukes of Hazzard,' died Monday at 88.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explaination of Code and Output:***\n",
        "\n",
        "This code snippet is designed to utilize few-shot learning with the Flan-T5 model to generate a text summary. The approach involves crafting a prompt that includes several examples, each consisting of an article and its summary, to instruct the model on the desired task. Here’s a breakdown of how the code works and why its output might be considered better than one-shot or zero-shot approaches:\n",
        "\n",
        "Code Functionality\n",
        "Environment Setup: The code starts by importing necessary modules and loading a tokenizer and a model, specifically the flan-t5-base model from the Transformers library by Hugging Face. This model is designed to handle language generation tasks.\n",
        "\n",
        "Model Preparation: The model is then set to run on a GPU if available, which is typical for deep learning tasks due to the computational power required.\n",
        "\n",
        "Prompt Construction: The code constructs a few-shot learning prompt that includes a couple of historical summary examples (about Albert Einstein and the Titanic) along with the current article about James Best. This prompt structure aims to teach the model the format and style expected in the summary.\n",
        "\n",
        "Tokenization and Model Inference: The prompt is tokenized and fed into the model, which then generates a summary based on the given input. The model is instructed to use settings like a maximum length limit and beam search to improve the quality of the output.\n",
        "\n",
        "Output Processing: The generated tokens are decoded back into text, and the system resources are cleaned up, particularly the GPU memory, to ensure efficient usage.\n",
        "\n",
        "Result Display: Finally, the baseline summary provided in the data and the model-generated summary are printed for comparison.\n",
        "\n",
        "Output Analysis and Comparison to One-Shot and Zero-Shot\n",
        "Baseline Summary: \"James Best, the sheriff on 'The Dukes of Hazzard,' died at 88.\"\n",
        "Few-Shot Generated Summary: \"James Best, the sheriff on 'The Dukes of Hazzard,' died Monday at 88.\"\n",
        "Why Is the Few-Shot Result Potentially Better?\n",
        "\n",
        "Contextual Learning: The few-shot learning approach allows the model to better understand the context and the specific task by seeing several related examples before attempting to generate the summary. This context can help the model tune its internal representations to the nuances of summarization tasks.\n",
        "\n",
        "Detail and Specificity: The model-generated summary includes the day of the week (\"Monday\"), which was not present in the baseline but might be inferred or retained from the detailed prompt. This inclusion shows the model’s ability to maintain specific details, which can be crucial for accurate text generation.\n",
        "\n",
        "Model Training: Flan-T5 is optimized for understanding and generating responses based on natural language prompts. The few-shot approach leverages this training by clearly setting expectations through examples, leading to more accurate and contextually appropriate outputs.\n",
        "\n",
        "Conclusion\n",
        "In comparison to one-shot (single example) or zero-shot (no examples given) approaches, few-shot learning can often result in more accurate and context-aware outputs. This is due to the model having more examples to \"learn from\" immediately before performing the task, helping it better understand the desired output format and content level. This method is especially useful when dealing with complex language tasks where subtleties in wording and context significantly impact the quality of the output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uNbIf9pRuyUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load tokenizer and model, appropriate for summarization\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Few-shot examples and the current article\n",
        "few_shot_examples = [\n",
        "    {\"article\": \"Steve Jobs, co-founder of Apple Inc., passed away in 2011 at the age of 56.\", \"summary\": \"Apple co-founder Steve Jobs died in 2011.\"},\n",
        "    {\"article\": \"James Best, who played the sheriff on 'The Dukes of Hazzard,' died Monday at 88. 'Hazzard' ran from 1979 to 1985 and was among the most popular shows on TV.\", \"summary\": \"James Best, the sheriff on 'The Dukes of Hazzard,' died at 88.\"}\n",
        "]\n",
        "\n",
        "# Prepare the prompt\n",
        "prompt = \"\\n\\n\".join([f\"Article: {ex['article']}\\nSummary: {ex['summary']}\" for ex in few_shot_examples])\n",
        "\n",
        "# Prepare input and ensure it is on the correct device\n",
        "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Generate output considering correct device usage\n",
        "with torch.no_grad():\n",
        "    output_tokens = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=512 + 50,  # Slightly larger to accommodate the summary\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "# Decode and print the output\n",
        "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "print('-' * 50)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
        "print('-' * 50)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{few_shot_examples[-1][\"summary\"]}\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf2mE9PtcA_m",
        "outputId": "a13fb856-cc09-436a-b698-12cc13ff0e12"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "MODEL GENERATION - FEW SHOT:\n",
            "James Best, the sheriff on 'The Dukes of Hazzard,' died Monday at 88.\n",
            "--------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "James Best, the sheriff on 'The Dukes of Hazzard,' died at 88.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explaination of Code and Output:***\n",
        "\n",
        "The code is a Python script that leverages the transformers library to use a pre-trained model, specifically google/flan-t5-base, for a few-shot learning summarization task. The script is designed to prepare and feed a structured prompt containing few-shot examples to the model, generate a summary based on this input, and then display the results.\n",
        "\n",
        "Key Steps in the Code:\n",
        "Environment Setup: The script begins by loading necessary modules and setting up the tokenizer and model from the Hugging Face transformers library.\n",
        "Device Configuration: It checks for GPU availability and moves the model to the GPU if available, which is crucial for performance optimization in model inference.\n",
        "Prompt Preparation: The script prepares a prompt that includes examples of article summaries, aimed at providing context to the model on how to perform the task of summarization.\n",
        "Input Preparation and Tokenization: The prepared prompt is tokenized and the tokenized input is moved to the appropriate device (GPU if available).\n",
        "Model Inference: The model generates a summary using the few-shot prompt, employing settings like beam search and no-repeat n-gram size to enhance the quality and coherence of the output.\n",
        "Output Processing and Display: The generated summary tokens are decoded back into text and displayed along with the baseline human summary for comparison.\n",
        "Output and Comparison to One-Shot Learning:\n",
        "The output displays the model-generated summary alongside a baseline human summary for the last example in the few-shot prompt. The model-generated summary for James Best is slightly more detailed than the human summary, including the day of his death (\"Monday\"), which isn't specified in the baseline summary.\n",
        "\n",
        "Differences from Previous Code:\n",
        "Contextual Learning via Few-Shot Examples: Unlike one-shot learning that typically relies on a single example to guide the model, this script uses multiple examples. This method is likely to improve the model's understanding and handling of the task by providing more context.\n",
        "Increased Accuracy and Detail: By providing more examples, the model can potentially learn better patterns and nuances of summarization, which might lead to more detailed and accurate outputs, as seen with the inclusion of the day \"Monday\" in the generated summary.\n",
        "Prompt Design: The structure of the prompt in few-shot learning is crucial as it directly influences how the model interprets and performs the task. This structured approach can lead to better performance compared to one-shot learning where the model has less contextual information.\n",
        "\n",
        "# **Conclusion:**\n",
        "\n",
        "In tasks like summarization, few-shot learning can be more effective than one-shot learning, particularly when using sophisticated language models like Flan-T5. The use of multiple examples helps the model generalize better over the task format and content, leading to potentially more accurate and informative summaries. This approach is beneficial when the task requires understanding complex patterns or when a higher quality of output is needed. In practical applications, few-shot learning can significantly enhance the model's performance by leveraging the additional examples to fine-tune its responses to specific tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KeM9jqWYvRg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Future Improvements***\n",
        "\n",
        "To enhance the performance of the code for generating summaries using few-shot learning with models like FLAN-T5 and to increase the size (length) of the output summaries, consider the following adjustments:\n",
        "\n",
        "1. Adjust Tokenization and Model Parameters\n",
        "Increase Max Length: You can increase the max_length parameter in the model.generate method. This parameter determines the maximum length of the generated sequence. By setting a higher value, you allow the model to generate longer summaries. However, keep in mind that increasing this might require more computational resources and could potentially lead to more verbose output that might not always add relevant information.\n",
        "\n",
        "Modify Beam Search Settings: Adjusting the num_beams parameter can improve the quality of the output. More beams might lead to better summaries as the model explores more possible sequences before deciding on the final output. This is a trade-off between computational cost and output quality.\n",
        "\n",
        "2. Enhance Few-Shot Prompt Design\n",
        "Diverse Examples: Include a wider range of examples in the few-shot prompt. Using examples from different domains or with varying styles can help the model better understand the nuances of summarization across contexts.\n",
        "\n",
        "Detailed Contextual Prompts: Instead of just appending articles and summaries, consider enhancing prompts with specific instructions or questions that guide the model more explicitly in generating detailed summaries. For instance, adding a line like, \"Please provide a detailed summary including key events, characters, and outcomes.\" can direct the model to focus on specifics.\n",
        "\n",
        "3. Experiment with Different Model Configurations\n",
        "Hyperparameter Tuning: Experiment with other generation parameters such as temperature, top_k, and top_p to control the randomness of the output and how conservatively the model samples phrases. Adjusting these can help in finding a balance between creativity and relevance in the generated summaries.\n",
        "\n",
        "Model Variants: Consider testing different model variants that might be more suited to generating longer text, such as flan-t5-large or even other models specialized in generating longer content.\n",
        "\n",
        "4. Utilize Advanced Decoding Techniques\n",
        "Length Penalty: Apply a length penalty in the generation settings to encourage longer outputs. The length_penalty parameter in the generate method can be adjusted to fine-tune the focus on longer sequences.\n",
        "\n",
        "Use of Stop Sequences: Define custom stop sequences to better control where the model ends its generation, ensuring it covers all necessary content before concluding.\n",
        "\n",
        "5. Post-Processing Enhancements\n",
        "Refinement Steps: After generating a summary, consider implementing a post-processing step where the summary is refined or extended using additional model prompts. This could involve re-summarizing or asking the model specific follow-up questions based on the initial summary.\n",
        "6. Continuous Evaluation and Feedback Loop\n",
        "Iterative Testing and Feedback: Continuously test the summaries with different types of articles and user feedback. Use this feedback to refine the prompts and model parameters iteratively.\n",
        "Conclusion\n",
        "By adjusting the model's generation parameters, enhancing the prompt design, and potentially using more advanced or larger models, you can significantly improve the quality and length of the generated summaries. These improvements should be balanced with performance considerations, as more complex models and longer outputs require more computational resources. Regularly updating and refining the approach based on testing and feedback can lead to progressively better performance in automated summarization tasks.\n"
      ],
      "metadata": {
        "id": "DKzrtUUuvgI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***References***\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/flan-t5 - Official documentation for FLAN-T5 on the Hugging Face Transformers library.\n",
        "https://arxiv.org/abs/1910.10683 - Original T5 model research paper by Google Research.\n",
        "https://arxiv.org/abs/2109.01652 - Research paper on \"Fine-tuned Language Models Are Zero-Shot Learners,\" which discusses FLAN-T5.\n",
        "https://www.tensorflow.org/tutorials/text/transformer - TensorFlow tutorial on building Transformer models for text processing.\n",
        "https://pytorch.org/hub/huggingface_pytorch-transformers/ - PyTorch hub page for the Hugging Face Transformers library.\n",
        "https://cloud.google.com/natural-language/docs/basics - Google Cloud's introduction to Natural Language Processing basics.\n",
        "https://nlp.stanford.edu/projects/glove/ - Stanford University's project page for GloVe, providing insights into word embeddings used in many NLP tasks.\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html - scikit-learn tutorial on working with text data.\n",
        "https://www.aclweb.org/anthology/ - A digital library of research papers in computational linguistics hosted by the Association for Computational Linguistics.\n",
        "https://www.nvidia.com/en-us/deep-learning-ai/industries/nlp/ - NVIDIA's resources on applying deep learning to Natural Language Processing."
      ],
      "metadata": {
        "id": "4YiAhVySv4Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**License for Jupyter Notebook**\n",
        "\n",
        "Title of Google Colaboratory: [CNNArticles_Summarization]\n",
        "\n",
        "Author: Shreya Bage\n",
        "\n",
        "Creation Date: [4/13/2024]\n",
        "\n",
        "License Effective Date: [4/13/2024]\n",
        "\n",
        "License Version: 1.0"
      ],
      "metadata": {
        "id": "8hxCDKdkwBys"
      }
    }
  ]
}
